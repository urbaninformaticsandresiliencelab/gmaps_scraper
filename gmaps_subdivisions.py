#!/usr/bin/env python

#from geopy.geocoders import GoogleV3
from math import radians, cos, sin, asin, sqrt
import cPickle as pickle
import googlemaps
import glob
import json
import os
import time
import sys

import parse_tiger
import geo
import staticmaps

# Attempt to load credentials
try:
    import credentials
    if (credentials.api_key == "Enter key here"):
        print("Error: no api key provided. Please add one to credentials.py.")
        sys.exit(1)
except:
    if (not os.path.isfile("credentials.py")):
        print("Error: could not find credentials.py. One has been created for you.")
    credentials_file = open("credentials.py", "w")
    credentials_file.write("# Google maps API key to be used by the scraper\n"
                           "api_key = \"Enter key here\"\n")
    credentials_file.close()
    sys.exit(1)
API_KEY = credentials.api_key

# There are 96 types of places that can be acquired
PLACE_TYPES = ["accounting","airport","amusement_park","aquarium","art_gallery","atm","bakery","bank","bar","beauty_salon","bicycle_store","book_store","bowling_alley","bus_station","cafe","campground","car_dealer","car_rental","car_repair","car_wash","casino","cemetery","church","city_hall","clothing_store","convenience_store","courthouse","dentist","department_store","doctor","electrician","electronics_store","embassy","fire_station","florist","funeral_home","furniture_store","gas_station","general_contractor","gym","hair_care","hardware_store","hindu_temple","home_goods_store","hospital","insurance_agency","jewelry_store","laundry","lawyer","library","liquor_store","local_government_office","locksmith","lodging","meal_delivery","meal_takeaway","mosque","movie_rental","movie_theater","moving_company","museum","night_club","painter","park","parking","pet_store","pharmacy","physiotherapist","plumber","police","post_office","real_estate_agency","restaurant","roofing_contractor","rv_park","school","shoe_store","shopping_mall","spa","stadium","storage","store","subway_station","synagogue","taxi_stand","train_station","transit_station","travel_agency","university","veterinary_care","zoo"]

# From https://developers.google.com/places/web-service/search: The maximum
# allowed radius is 50000 meters.
MAX_RADIUS_METERS = 50000

# No further subdivisions under this size will be made
MIN_RADIUS_METERS = 5

# The length of a period, in seconds
PERIOD_LENGTH = 60*60 # One hour

# Maximum number of requests that can be made in one period
MAX_REQUESTS_PER_PERIOD = 5000

# Time, in seconds, to sleep between each request
REQUEST_DELAY = 1.5

# Maximum number of times a request can be retried
MAX_RETRIES = 5

# Files that will be written to (names are changed later)
OUTPUT_DIRECTORY_ROOT = "output/raw_pickle/" # The top level output directory

# Google Maps API client object
gmaps = googlemaps.Client(
    key = API_KEY,
    timeout = 600
)

# Main scraper class contains functionality for initialization and setting of
# output directory, logging, and rate limiting
class Scraper(object):
    """ Main scraper class

    Contains functionality for initialization and management of the output
    directory, logging, and rate limiting. Rate limiting is done by defining
    periods of a fixed length and a maximum number of requests per period which
    can not be exceeded; the script sleeps until the next period if the limit
    is reached.

    The rate_limit method must be called once before every request and handles
    the rate limiting by tracking the number of requests and time since the
    current period started.

    Attributes:
        output_directory_name: A string containing the name of the root
            directory containing all output generated by the scraper.
        output_directory: A string containing the name of the subdirectory of
            output_directory containing all coutput generated by the scraper in
            the current scraping period.
        start_time: A float of the Unix time when the scraper was initialized.
        request_period_start_time: A float of the Unix time when the current
            period was started.
        traversed: An integer indicating how many pages were traversed since the
            scraper started.
        traversed_this_period: An integer indicating how many pages were
            traversed since the current period was started.
    """

    def __init__(self, output_directory_name = "Untitled_Scrape"):
        """ Initializes Scraper class

        Performs necessary initialization before the scraper starts running,
        including recording the program start time and initializing an output
        directory.

        Args:
            output_directory_name: A directory which is a subdivision of
                OUTPUT_DIRECTORY_ROOT where scraped data and logs will be
                stored.
        """
        self.output_directory_name = output_directory_name
        self.output_directory = ""

        self.start_time = time.time()

        self.request_period_start_time = time.time()
        self.traversed = 0
        self.traversed_this_period = 0

        self.initialize_output_directory()

    def initialize_output_directory(self):
        """ Initializes an output directory for the current period

        Creates a directory in which all data and logs generated by the scraper
        in the current period will be stored. The name of the directory is an
        ISO-formatted timestamp corresponding to when the period was started.

        Additionally, blank log files with headers are created.
        """
        self.output_directory = "%s/%s/%s" % (
            OUTPUT_DIRECTORY_ROOT,
            self.output_directory_name, time.strftime("%Y-%m-%dT%H:%M:%S")
        )
        os.makedirs(self.output_directory)

        print("Writing data and logs to %s/" % self.output_directory)

        # Initialize logs
        logs = {
            "request_log.csv": "REQUESTS",
            "error_log.csv": "ERROR",
            "termination_log.csv": "REASON"
        }
        for log in logs.iterkeys():
            _file = open("%s/%s" % (self.output_directory, log), "w")
            _file.write("TIME,%s\n" % logs[log])
            _file.close()

    def log(self, filename, message):
        """ Write a timestamped message to a log

        Timestamps are floating points that indicate the amount of time since
        the current period was started.

        Args:
            filename: A string containing the name of the log to be written to.
            message: A string containing the message to be logged.
        """
        _file = open("%s/%s" % (self.output_directory, filename), "a")
        _file.write("%f,%s\n" % (time.time() - self.start_time, message))
        _file.close()

    def rate_limit(self):
        """ Self-imposed rate limiting functionality

        THIS MUST BE RUN ONCE BEFORE EVERY REQUEST!!!!

        Limits the number of requests made by holding up the script if the
        number of requests made this period exceeds the number defined by the
        MAX_REQUESTS_PER_PERIOD global variable.
        """
        current_period_length = time.time() - self.request_period_start_time

        while ((current_period_length < PERIOD_LENGTH)
               and (self.traversed_this_period >= MAX_REQUESTS_PER_PERIOD)):
            print("Max requests per period reached (%d). %f Seconds until next period." % (
                MAX_REQUESTS_PER_PERIOD, PERIOD_LENGTH - current_period_length))
            time.sleep(10)

        # End of period
        if ((time.time() - self.request_period_start_time) >= PERIOD_LENGTH):

            # Reset variables
            self.request_period_start_time = time.time()
            self.traversed_this_period = 0

            # For analysis: output time since program started and total requests
            # made during this period
            self.log("request_log.csv", self.traversed)

            # Create a new output directory
            self.initialize_output_directory()

        # Increment the counters
        self.traversed += 1
        self.traversed_this_period += 1

    def dump_data(self, data):
        """ Dump data to a pickle

        Args:
            data: The data structure to be dumped
        """

        output_file_object = open(self.output_directory + "/data.p", "a+b")
        pickle.dump(data, output_file_object)
        output_file_object.close()

class DetailScraper(Scraper):
    """ Subclass of Scraper that specifically scrapes place details

    Attributes:
        dump_interval
    """

    def __init__(self, output_directory_name, dump_interval = 50):
        """ Initializes DetailScraper class

        Args:
            output_directory_name: A directory which is a subdivision of
                OUTPUT_DIRECTORY_ROOT where scraped data and logs will be
                stored.
            dump_interval: An integer representing the number of place_ids
                traversed between each dump
        """

        Scraper.__init__(self, output_directory_name)
        self.dump_interval = dump_interval

    def scrape(self, target):
        """ The main function of DetailScraper

        Scrapes a list of place_ids using the Google Maps API's place details
        API.

        Args:
            target: One of the following:
                * A string containing the path to a JSON file which has been
                  created by process_output.py.
                * A string containing a single place_id
                * A list or tuple containing place_ids
        """

        place_ids = []
        results = []
        counter = 1

        if (type(target) is str):
            if (os.path.isfile(target)) and (target[-5:] == ".json"):
                file_object = open(target)
                for datum in json.load(file_object):
                    place_ids.append(datum["place_id"])
                file_object.close()
                print("Added %d place_ids from %s" % (len(place_ids), target))
            else:
                place_ids.append(target)
        elif (type(target) is list) or (type(target) is tuple):
            if (type(target[0]) is str):
                place_ids += target

        if (len(place_ids) == 0):
            raise Exception("Invalid target supplied")

        num_place_ids = len(place_ids)
        for place_id in place_ids:

            self.rate_limit() ##################################################

            # Dump results periodically
            if ((counter % self.dump_interval) == 0):
                print("Dumping last %d results" % self.dump_interval)
                self.dump_data(results)
                results = []

            print("Scraping place_id %s (%d/%d - %0.3f%%)" % (
                place_id, counter, num_place_ids,
                float(counter)/num_place_ids*100,
            ))

            for attempt in range(MAX_RETRIES):
                try:
                    results.append(gmaps.place(place_id)["result"])
                    time.sleep(REQUEST_DELAY)
                    break
                except Exception as err:
                    print("Error: %s" % err)
                    # For analysis: output time since program started and the text of the
                    # error
                    self.log("error_log.csv", err)

                    time.sleep(REQUEST_DELAY)
                    pass
                print("Retrying (attempt #%d)" % (attempt + 1))

            if (attempt == MAX_RETRIES - 1):
                print("Max retries exceeded; skipping this place_id.")
                self.log(
                    "termination_log.csv",
                    ("Maximum number of retries exceeded. place_id: %s" % place_id)
                )

            counter += 1

        # Dump remaining results to a pickle file
        self.dump_data(results)

class PlaceScraper(Scraper):
    """ Subclass of Scraper specifically for scraping places using subdivisions

    A subclass of the Scraper class that contains functionality for scraping all
    of the places in a given area by scraping subdivisions of that area.

    Attributes:
        scrape = A pointer to the main scraping function. All scraping functions
            return an array and take have following arguments:
                latitude: The center latitude of the scrape area.
                longitude: The center longitude of the scrape area.
                radius_meters: The radius, in meters, of the scraping area.
                place_type: A string containing the place_type to be scraped.
                subdivision_id_string: A string detailing the ancestry of the
                    current scrape area in the subdivision tree.
        max_results = A digit that describes the maximum number of results that
            the main scraping function can return, as defined by the Google Maps
            API documentation.
        gsm: a staticmaps.Constructor object used for generating Google Static
            Maps API links.
    """

    def __init__(self, output_directory_name, scrape_type = "places_nearby"):
        """ Initializes PlaceScraper class

        Args:
            output_directory_name: A directory which is a subdivision of
                OUTPUT_DIRECTORY_ROOT where scraped data and logs will be
                stored.
            scrape_type: A string describing which requests to use, which may
                be either places_nearby or places_radar
        """

        Scraper.__init__(self, output_directory_name)

        if (scrape_type == "places_nearby"):
            self.scrape = self.scrape_places_nearby
            self.max_results = 60
        elif (scrape_type == "places_radar"):
            self.scrape = self.scrape_places_radar
            self.max_results = 200
        else:
            print("Fatal: \"%s\" is not a valid scrape type" % scrape_type)

        self.gsm = staticmaps.Constructor()

        print("Configured scraper to scrape \"%s\"; max results = %d" % (
            scrape_type, self.max_results
        ))

    def scrape_places_radar(self, latitude, longitude, radius_meters,
                            place_type, subdivision_id_string):
        """ Get points of interest from the Google Maps API using the radar

        Attempt to download all places in the given scrape area using the
        places_radar function of the Google Maps API library. A maximum of
        MAX_RETRIES attempts are made before the function gives up and logs the
        branch termination to termination_log.csv.

        Args:
            See the "scrape" attribute above.

        Returns:
            An array containing places returned by the Google Maps API function.

            A blank array is returned if MAX_RETRIES attempts were made.
        """

        self.rate_limit() ######################################################

        results = []

        for attempt in range(MAX_RETRIES):
            try:
                results = gmaps.places_radar(
                    location = {
                        "lat": latitude,
                        "lng": longitude
                    },
                    radius = radius_meters,
                    type = place_type
                )["results"]
                time.sleep(REQUEST_DELAY)
                break
            except Exception as err:
                print("Error: %s" % err)
                # For analysis: output time since program started and the text of the
                # error
                self.log("error_log.csv", err)

                time.sleep(REQUEST_DELAY)
                pass
            print("Retrying (attempt #%d)" % (attempt + 1))

        if (attempt == MAX_RETRIES - 1):
            print("Max retries exceeded; skipping this subdivision.")
            self.log(
                "termination_log.csv",
                ("Maximum number of retries exceeded. Subdivision ID: %s. Place type: %s. Coordinates: (%f, %f). Radius: %f" % (
                    subdivision_id_string,
                    place_type,
                    latitude,
                    longitude,
                    radius_meters
                ))
            )

        return results

    def scrape_places_nearby(self, latitude, longitude, radius_meters,
                             place_type, subdivision_id_string,
                             page = 1, retries = 0, token = "none"):
        """ Get points of interest from the Google Maps API using places_nearby

        Attempt to download all places in the given scrape area using the
        places_nearby function of the Google Maps API library. A maximum of
        MAX_RETRIES attempts are made before the function gives up and logs the
        branch termination to termination_log.csv.

        Because of how the API functions, scrape_places_nearby recurses if a
            next_page_token is returned in the results array.

        Args:
            See the "scrape" attribute above.
            page: An integer describing the number of pages traversed as of the
                current recursion.
            retries: An integer describing the number of attempts made as of the
                current recursion.
            token: A string containing a token that can be used to request the
                next page of results, passed by the previous recursion.

        Returns:
            An array containing places returned by the Google Maps API function.

            A blank array is returned if MAX_RETRIES attempts were made.
        """

        self.rate_limit() ######################################################

        combined_results = []

        print("Retrieving page %d" % page)
        try:
            # Only provide a page_token if the next_page_token was provided
            if (token == "none"):
                results = gmaps.places_nearby(
                    location = {
                        "lat": latitude,
                        "lng": longitude
                    },
                    radius = radius_meters,
                    type = place_type
                )
            else:
                results = gmaps.places_nearby(
                    location = {
                        "lat": latitude,
                        "lng": longitude
                    },
                    radius = radius_meters,
                    type = place_type,
                    page_token = token
                )

            # From https://developers.google.com/places/web-service/search:
            # "next_page_token contains a token that can be used to return up to
            # 20 additional results. A next_page_token will not be returned if
            # there are no additional results to display."
            # If the next_page_token exists, recurse and append to the
            # combined_results array.
            time.sleep(REQUEST_DELAY)
            if "next_page_token" in results:
                token = results["next_page_token"]
                combined_results += self.scrape_places_nearby(latitude, longitude,
                                                              radius_meters,
                                                              place_type,
                                                              subdivision_id_string,
                                                              page + 1, retries,
                                                              token)

            combined_results += results["results"]

        except Exception as err:
            print("Error: %s" % err)
            self.log("error_log.csv", err)

            time.sleep(REQUEST_DELAY)

            if (retries <= MAX_RETRIES):
                print("Retrying (attempt #%d)" % (retries + 1))
                combined_results += self.scrape_places_nearby(latitude, longitude,
                                                              radius_meters,
                                                              place_type,
                                                              subdivision_id_string,
                                                              page, retries + 1, token)
            else:
                print("Max retries exceeded; skipping this subdivision.")
                self.log(
                    "termination_log.csv",
                    ("Maximum number of retries exceeded. Subdivision ID: %s. Place type: %s. Coordinates: (%f, %f). Radius: %f" % (
                        subdivision_id_string,
                        place_type,
                        latitude,
                        longitude,
                        radius_meters
                    ))
                )

            pass

        return combined_results

    def scrape_subdivisions(self, min_latitude, max_latitude, min_longitude,
                            max_longitude, grid_width, place_type,
                            subdivision_parent_id = "root",
                            target_subdivision_id = ""):
        """ Recursive function that creates subdivisions and invokes the scraper

        This is the main function that manages the creation of subdivisions and
        invokes the scraper to scrape places from those subdivisions.

        Subdivisions are congruent rectangular researches which are similar to
        the region defined in the function's arguments. All subdivisions have
        the same width and height and are arranged in a square grid.

        For example, a 3x3 grid of subdivisions with min_lat = 0, min_long = 0,
        max_lat = 3, max_long = 3 would look like this:

            increasing latitude
            ^
            3 +---+---+---+
              | 7 | 8 | 9 |
            2 +---+---+---+
              | 4 | 5 | 6 |
            1 +---+---+---+
              | 1 | 2 | 3 |
            0 +---+---+---+
              0   1   2   3 > increasing longitude

        Where the number inside each box corresponds to the order in which that
        cell is processed.

        To calculate the scraping area, which is circular:
            We use the Pythagorean theorem to find the diameter of the smallest
                circle containing all points of the scraping area and divide
                that by 2 to get the radius.
            We get the average longitude and latitude to get the center.

        The scraper function is called on each cell and each cell is further
        subdivided into another square grid of congruent cells if the maximum
        number of results, as defined in the initialization, is returned. To do
        this, the function recurses with the cell's region becoming the new
        grid's region.

        Each cell is assigned a string detailing that cell's ancestry. For
        example, the bottom left subdivision of the top right subdivision of the
        root cell has the ID "root -> 9 -> 1".

        Args:
            min_latitude, max_latitude, min_longitude, max_longitude: Floating
                points describing the bounds of the scraping region
            grid_width: An integer describing the number of rows and columns to
                subdivide the scraping region into. The total number of cells is
                grid_width^2.
            place_type: A string containing the place_type to be scraped
            subdivision_parent_id: A string containing the subdivision ID of the
                parent region. If there is no parent region, the ID is "root".
                This variable is managed by the function and should not be
                modified externally.
            target_subdivision_id: An optional string containing the subdivision
                ID of the cell to be skipped to. If defined, the function will
                only scrape subdivisions of that cell; all previous cells are
                ignored and the function terminates after all of that cell's
                subdivisions have been scraped.
        """

        subdivision_id = 0
        subdivision_width = (max_latitude - min_latitude)/grid_width
        subdivision_height = (max_longitude - min_longitude)/grid_width

        # If a target subdivision ID is supplied, skip to that subdivision
        target_subdivision = 0
        split_id = target_subdivision_id.split(" -> ")
        if (target_subdivision_id != ""):
            if (target_subdivision_id[:4] == "root"):
                print("Skipping forward to %s\n" % target_subdivision_id)
                target_subdivision = int(split_id[1])
                split_id = split_id[2:]
            else:
                target_subdivision = int(split_id[0])
                split_id = split_id[1:]
        target_subdivision_id = " -> ".join(split_id)

        for row in range(grid_width):
            for column in range(grid_width):
                # The subdivision ID is used to track the current subdivision's
                # ancestry. To find exactly where on a grid this subdivision
                # lies, use the above table.
                subdivision_id += 1
                subdivision_id_string = (subdivision_parent_id + " -> "
                                         + str(subdivision_id))

                # If a target subdivision is specified, skip all of the below
                # logic and continue to the next loop
                if (target_subdivision != 0):
                    if (target_subdivision == subdivision_id):
                        print("Skipped to %s" % subdivision_id_string)
                    else:
                        continue

                # First, we need to establish the bounds of this subdivision
                subdivision_min_latitude = (min_latitude
                                            + (subdivision_width * float(row)))
                subdivision_max_latitude = (subdivision_min_latitude
                                            + subdivision_width)
                subdivision_min_longitude = (min_longitude
                                             + (subdivision_height * float(column)))
                subdivision_max_longitude = (subdivision_min_longitude
                                            + subdivision_height)

                # Then, we can establish the center and the radius of the circle
                # needed to encompass the entire subdivision
                subdivision_center_longitude = ((subdivision_min_longitude
                                                 + subdivision_max_longitude)/2)
                subdivision_center_latitude = ((subdivision_min_latitude
                                                 + subdivision_max_latitude)/2)

                # The haversine formula is used to convert the width and height
                # from degrees into meters before finding the radius in meters
                width_meters = geo.haversine(0, subdivision_min_longitude,
                                             0, subdivision_max_longitude)
                height_meters = geo.haversine(0, subdivision_min_latitude,
                                              0, subdivision_max_latitude)

                # From there, we use the pythagorean theorem to find the radius
                subdivision_radius_meters = (sqrt((width_meters/2)**2
                                                  + (height_meters/2)**2))

                # This bool will be changed to true if more recursions are
                # necessary
                make_subdivisions = False

                if (len(split_id) == 0) or ("" in split_id):
                    print("Subdivision ID: %s" % subdivision_id_string)
                    print("Center coords: (%f, %f)" % (subdivision_center_latitude,
                                                       subdivision_center_longitude))
                    print("Radius: %f meters" % subdivision_radius_meters)
                    print("Extents: ")
                    print({
                        "min_longitude": subdivision_min_longitude,
                        "max_longitude": subdivision_max_longitude,
                        "min_latitude": subdivision_min_latitude,
                        "max_latitude": subdivision_max_latitude
                    })
                    self.gsm.add_coords([
                        [subdivision_min_longitude, subdivision_min_latitude],
                        [subdivision_max_longitude, subdivision_min_latitude],
                        [subdivision_max_longitude, subdivision_max_latitude],
                        [subdivision_min_longitude, subdivision_max_latitude]
                    ], "polygon")
                    print("Visualization: %s" % self.gsm.generate_url())
                    self.gsm.reset()

                    # If the radius of the subdivision exceeds the max, skip the result
                    # collection and recurse
                    if (subdivision_radius_meters > MAX_RADIUS_METERS):
                        print("Making subdivisions because radius exceeded maximum")
                        make_subdivisions = True

                    elif (subdivision_radius_meters < MIN_RADIUS_METERS):
                        print("Terminating branch because radius is below the minimum")
                        self.log(
                            "termination_log.csv",
                            ("Radius fell below minimum value. Subdivision ID: %s. Place type: %s. Coordinates: (%f, %f). Radius: %f" % (
                                subdivision_id_string,
                                place_type,
                                subdivision_center_latitude,
                                subdivision_center_longitude,
                                subdivision_radius_meters)
                            )
                        )

                    else:

                        # Get results
                        results = self.scrape(subdivision_center_latitude,
                                              subdivision_center_longitude,
                                              subdivision_radius_meters,
                                              place_type,
                                              subdivision_id_string)
                        print("%d results for place_type %s" % (len(results),
                                                                place_type))
                        print("%d pages traversed since program was started"
                              % self.traversed)

                        # Save the results in a pickle file
                        self.dump_data(results)

                        # If 60 results were returned, recurse
                        if (len(results) == self.max_results):
                            print("Making subdivisions because max results were returned")
                            make_subdivisions = True

                else:
                    make_subdivisions = True

                # Recurse if necessary
                if (make_subdivisions):
                    print
                    self.scrape_subdivisions(subdivision_min_latitude,
                                              subdivision_max_latitude,
                                              subdivision_min_longitude,
                                              subdivision_max_longitude,
                                              3, place_type,
                                              subdivision_id_string,
                                              target_subdivision_id)
                else:
                    print("Branch terminated\n")

## Program Initialization ######################################################

if (__name__ == "__main__"):
    from optparse import OptionParser

    parser = OptionParser()
    parser.add_option("--state", dest = "state", metavar = "STATE",
                      help = "Use the shapefile located in tiger-2016/STATE",
                      default = "null")
    parser.add_option("--city", dest = "city", metavar = "CITY",
                      help = "Scrape the CITY shape in the chosen shapefile",
                      default = "null")
    parser.add_option("--type", dest = "type", metavar = "TYPE",
                      help = "Use a scraper of the specified TYPE. Types: "
                             "places_nearby, places_radar")
    parser.add_option("--outdir", dest = "outdir", metavar = "OUTDIR",
                      help = "Write all results to subdirectories of OUTDIR")
    (options, args) = parser.parse_args()

    if (not os.path.isdir("tiger-2016/" + options.state)):
        print("Please specify a valid state with --state. See --help for more "
              "info. Possible states:")
        print(", ".join(sorted(os.listdir("tiger-2016/"))))
        sys.exit(1)
    state_shapefile = glob.glob("tiger-2016/" + options.state + "/*.shp")[0]

    city_extents = parse_tiger.get_extents(state_shapefile, options.city)
    if (not city_extents):
        print("Please specify a valid city with --city. See --help for more "
              "info. Possible states:")
        print(", ".join(sorted(parse_tiger.dump_names(state_shapefile))))
        sys.exit(1)

    if (options.type != "places_radar") and (options.type != "places_nearby"):
        print("Please specify a scrape type with --type. See --help for more "
                "info.\nPossible types: places_nearby, places_radar")
        sys.exit(1)

    if (options.outdir):
        scraper_output_directory_name = options.outdir
    else:
        scraper_output_directory_name = ("%s_%s_%s_%s" % (
                                            time.strftime("%Y-%m-%d"),
                                            options.city, options.state,
                                            options.type
                                        )).replace(" ", "_")

    print
    new_scraper = PlaceScraper(scraper_output_directory_name, options.type)
    print

    # For each place_type, the subdivision -> extraction process is restarted
    # from scratch.
    for place_type in PLACE_TYPES:
        new_scraper.scrape_subdivisions(city_extents["min_latitude"],
                                        city_extents["max_latitude"],
                                        city_extents["min_longitude"],
                                        city_extents["max_longitude"],
                                        3, place_type)

    print("Finished scraping %s, %s" % (options.city, options.state))
