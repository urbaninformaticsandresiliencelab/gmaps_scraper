<h1 id="gmaps-scraper">gmaps-scraper</h1>
<p><em>gmaps-scraper</em> is a suite of tools designed to facilitate the scraping and processing of place data using the Google Maps API.</p>
<h1 id="table-of-contents">Table of Contents</h1>
<h2 id="code-overview">Code Overview</h2>
<p>For scraping:</p>
<ul>
<li>gmaps_subdivisions.py - The main scraping script.</li>
</ul>
<p>For processing:</p>
<ul>
<li>process_pickles.py - A script to automate the archiving of pickles and the creation of merged and deduplicated JSONs.</li>
<li>create_json_parallel_redis.py - A script to process a single pickle collection very fast by using Redis as a shared memory store for multiple worker processes.</li>
</ul>
<p>Additional libraries used by the scraper:</p>
<ul>
<li>geo.py - A library providing various geometric functions such as the haversine formula, the law of cosines, and a function for point-in-polygon.</li>
<li>gms_io.py - A library providing various classes that handle the writing of scraped data to various formats.</li>
<li>parse_tiger.py - A library providing wrapper functions for parsing the US Census TIGER data by using the shapefile library.</li>
<li>staticmaps.py - A library that generates valid Google Static Maps API URLs for visualizing areas on Google Maps.</li>
</ul>
<p>Utility scripts:</p>
<ul>
<li>scrape_tiger.sh - Scrape and process US Census TIGER data for use by the various scripts.</li>
</ul>
<h2 id="setup">Setup</h2>
<h2 id="usage">Usage</h2>
<ul>
<li>Scraping</li>
<li>Processing</li>
<li>Using grep to Count Occurances of Patterns</li>
</ul>
<h1 id="code-overview-1">Code Overview</h1>
<p>Scripts are documented internally and formatted in accordance with <a href="https://google.github.io/styleguide/pyguide.html">Google's Python styleguide</a>. For more in-depth documentation, refer to the comments and docstrings located in each script.</p>
<h2 id="gmaps_subdivisions.py">gmaps_subdivisions.py</h2>
<p><em>gmaps_scraper.py</em> scrapes places from Google Maps by subdividing a chosen area into smaller areas and making a requests for places in that area, making further subdivisions if a certain threshold is met, by default the maximum number of results defined by the <a href="https://developers.google.com/places/web-service/search">Google Places API Web Service documentation</a> with a little bit of head space (50 instead of 60, as of the time of writing this). Additionally, a bug was discovered where places_nearby searches over a large area with more than 60 results would return less than 60 results, so the threshold is automatically relaxed for the first few subdivisions in the hierarchy using the formula</p>
<pre><code>ADJUSTED_THRESHOLD = ACTUAL_THRESHOLD * (1 - 0.6 / HIERARCHY_DEPTH)</code></pre>
<p>More information about the scraping algorithm is documented in the <em>scrape_subdivisions</em> method of the <em>SubdivisionScraper</em> class. From the script:</p>
<pre><code>This is the main function that manages the creation of subdivisions and
invokes the scraper to scrape places from those subdivisions.

Subdivisions are congruent rectangular areas which are similar to the
region defined in the function&#39;s arguments. All subdivisions have the
same width and height and are arranged in a square grid.

For example, a 3x3 grid of subdivisions with min_lat = 0, min_long = 0,
max_lat = 3, max_long = 3 would look like this:

    increasing latitude
    ^
    3 +---+---+---+
      | 7 | 8 | 9 |
    2 +---+---+---+
      | 4 | 5 | 6 |
    1 +---+---+---+
      | 1 | 2 | 3 |
    0 +---+---+---+
      0   1   2   3 &gt; increasing longitude

Where the number inside each box corresponds to the order in which that
cell is processed.

To calculate the scraping area, which is circular:
    We use the Pythagorean theorem to find the diameter of the smallest
        circle containing all points of the scraping area and divide
        that by 2 to get the radius.
    We get the average longitude and latitude to get the center.

The scraper function is called on each cell and each cell is further
subdivided into another square grid of congruent cells if the threshold,
as defined in the initialization, is met. To do this, the function recurses
with the cell&#39;s region becoming the new grid&#39;s region.

Each cell is assigned a string detailing that cell&#39;s ancestry. For
example, the bottom left subdivision of the top right subdivision of the
root cell has the ID &quot;root -&gt; 9 -&gt; 1&quot;.</code></pre>
<p>Because each cell in the entire scraping tree is assigned a unique ID, we can later re-scrape cells that have been abandoned due to exceeding the maximum number of retries, as defined by the <em>MAX_RETRIES</em> constant, or cells that exhibit strange behaviour:</p>
<pre><code>To re-scrape a subdivision, all arguments except subdivision_parent_id
must be supplied.</code></pre>
<p><em>gmaps_scraper.py</em> provides the following classes:</p>
<ul>
<li>Scraper: A class for building generic Google Maps API scrapers
<ul>
<li>DetailScraper: A child class of Scraper built for scraping place details</li>
<li>SubdivisionScraper: A child class of Scraper meant for building scrapers that use the subdivisions algorithm, defined above.
<ul>
<li>PlacesNearbyScraper: A child class of SubdivisionScraper built for scraping the places_nearby API call.</li>
<li>PlacesRadarScraper: A child class of SubdivisionScraper built for scraping the places_radar API call. This currently has buggy behaviour and PlacesNearbyScraper should be used instead.</li>
<li>PlacesTextScraper: A child class of SubdivisionScraper built for scraping the places_radar API call, filtering results by using a given keyword.</li>
</ul></li>
</ul></li>
</ul>
<p>The writing of scraped data is handled by <em>gms_io.py</em>, which has the ability to deduplicate on the fly. More information can be found under the <em>gms_io.py</em> section.</p>
<h2 id="process_pickles.py">process_pickles.py</h2>
<p>Due to the nature of the scraper, the output pickle files, which are appended to after every successful request, contain a massive amount of duplicates. The <em>process_pickles.py</em> script is responsible for automating the deduplication of scrape data. The script:</p>
<ul>
<li>Obtains a list of all scrapes in <em>gmaps_subdivisions.py</em>'s output directory by using the glob library.</li>
<li>For each scrape, merge and deduplicate the pickled data if a JSON does not exist yet.</li>
<li>For each scrape, compress the pickled data into a .tar.xz file if one does not exist yet.</li>
</ul>
<p>The script utilizes Python's multiprocessing library to make fuller use of system resources by doing multiple merges and compressions at the same time. The number of worker processes is defined by the <em>THREADS</em> constant, which is, by default, 3.</p>
<h2 id="create_json_parallel_redis.py">create_json_parallel_redis.py</h2>
<p><em>create_json_parallel_redis.py</em> takes advantage of Redis' ability to serve as a very fast and light cache to speed up the merging and deduplication of a single scrape by starting multiple worker processes on different pickles of the same scrape, using Redis as shared dictionary of already-seen place IDs. For deduplication, scripts make use of Redis' <a href="https://redis.io/commands/get">Get</a> and <a href="https://redis.io/commands/set">Set</a> functions, which are both atomic and of O(1) complexity. As was the case in process_pickles.py, the number of worker processes is defined by the <em>THREADS</em> constant, which is, by default, 4.</p>
<h2 id="geo.py">geo.py</h2>
<p><em>geo.py</em> is a small library providing primitive geometric functions that are used in <em>gmaps_subdivisions.py</em>. Functions included:</p>
<ul>
<li>point_in_polygon - A function that returns True if a point is in a polygon and False if otherwise.</li>
<li>haversine and law_of_cosines - Calculate the distance between two points on a sphere.</li>
</ul>
<h2 id="gms_io.py">gms_io.py</h2>
<p><em>gms_io.py</em> handles the saving of scraped data to various file formats or databases and provides two families of classes: duplicate checkers and writers.</p>
<p>Duplicate checkers have two methods: <em>check</em>, which checks to see if a place has already been saved, and <em>flush</em>, which clears the list of seen places. These are used by writer classes, which have a single dump method that takes an array of dictionaries as input and saves the given dictionaries to an output destination.</p>
<p>Duplicate checker classes provided: * DuplicateChecker: The base class to use when no other classes can be instanced or duplicate checking is not desired. This mimics the behaviour of other duplicate checkers but does not actually do any checking. * SQLite3DuplicateChecker: A duplicate checker that checks against an SQLite database. * RedisDuplicateChecker: A duplicate checker that checks against a Redis set.</p>
<p>Writer classes provided: * Writer: Base writer class that provides no functionality other than the initialization of a duplicate checker. * MongoWriter: Handles writing to a MongoDB collection. * PickleWriter: Handles writing to a pickle files, separated by period. This was previously the default &quot;writer&quot; of <em>gmaps_scraper.py</em>. * JSONWriter: Handles writing to a JSON file.</p>
<h2 id="parse_tiger.py">parse_tiger.py</h2>
<p><em>parse_tiger.py</em> provides simple wrapper operations tailored for processing US Census TIGER shapefiles. Functions included:</p>
<ul>
<li>dump_names - Return an array of all places included in a shapefile.</li>
<li>dump_points - Return an array of all points included in a shapefile. This can be narrowed down to a single city.</li>
<li>get_extents - Return the most extreme coordinates of a shapefile. This can be narrowed down to a single city.</li>
</ul>
<h2 id="staticmaps.py">staticmaps.py</h2>
<p><em>staticmaps.py</em> provides a Constructor class which is used to generate valid Google Static Maps API URLs. Class methods:</p>
<ul>
<li>generate_url - Combine stored shapes into a single URL.</li>
<li>add_coords - Add coordinates to the current static map in the form of individual markers, a path, or a polygon..</li>
<li>reset - Remove all stored shapes.</li>
</ul>
<h2 id="scrape_tiger.sh">scrape_tiger.sh</h2>
<p><em>scrape_tiger.sh</em> is a script that is run once in the setup to scrape and process US Census TIGER data. The script downloads data, decompresses it, and organizes it according to the name of the state that each archive contains.</p>
<h1 id="setup-1">Setup</h1>
<p>Before doing anything, you must first cd into this directory. Once that is done, run <em>scrape-tiger.sh</em>, which will scrape the TIGER data, placing it into a directory named <em>tiger-2016-src/</em>:</p>
<pre><code>./scrape-tiger.sh</code></pre>
<p>It will then process this data, placing the organized data in a directory named <em>tiger-2016/</em>. At this point, <em>tiger-2016-src/</em> can be removed if you wish.</p>
<p>The next step is to create a <em>credentials.py</em> if you do not already have one. To create a new one, run <em>gmaps_scraper.py</em> once; it should create a template for you:</p>
<pre><code>./gmaps_scraper.py</code></pre>
<p>Enter the appropriate keys and save the file.</p>
<h1 id="usage-1">Usage</h1>
<p>As in the setup, you must cd into this directory before running any of the scripts.</p>
<h2 id="scraping">Scraping</h2>
<p>To scrape a city, supply the necessary arguments to <em>gmaps_scraper.py</em>. More information on what arguments to supply can be viewed by passing the <em>--help</em> argument:</p>
<pre><code>./gmaps_scraper.py --help</code></pre>
<p>For example, to scrape Boston, Massachusetts using the places_nearby scraper:</p>
<pre><code>./gmaps_scraper.py --type places_nearby --city Boston --state Massachusetts</code></pre>
<h2 id="processing">Processing</h2>
<p>As stated in the Code Overview section, there are two scripts that can merge and deduplicate the pickled scrape data, outputting JSON files. The simplest thing to do is to run the <em>process_pickles.py</em> script, which will automate all merging, deduplicating, and archiving:</p>
<pre><code>./process_pickles.py</code></pre>
<p>For particularly large scrapes, such as those of New York City or Los Angeles, you may prefer to use <em>create_json_parallel_redis.py</em>. To use this script, you must <a href="https://redis.io/download">download and build Redis</a> or install it using your distribution's package manager. After you have <em>redis-server</em> up and running, you can run the script, which will prompt you to choose a single scrape directory to work with:</p>
<pre><code>./create_json_parallel_redis.py</code></pre>
<p>After the script finishes, you may want to remove the <em>dump.rdb</em> file created in whatever directory you ran <em>redis-server</em> from. The database is cleared before each merge and deduplication, so there is no merit to keeping this file.</p>
<h2 id="using-grep-to-count-occurances-of-patterns">Using grep to Count Occurances of Patterns</h2>
<p>A very fast and convenient way to count how many times a pattern appears, such as &quot;atm&quot; if you want to find the number of ATMs in a scrape or &quot;place_id&quot; if you want to find the number of unique places in a scrape, is to use GNU grep from the GNU coreutils. To quickly count the number of times something occurs, use the following syntax:</p>
<pre><code>grep PATTERN FILE | wc -l</code></pre>
<p>Where <em>PATTERN</em> is a regular expression that you want to find and <em>FILE</em> is the file to be searched. For example, to find the number of ATMs in Boston:</p>
<pre><code>grep atm output/json/2016-12-26_Boston_Massachusetts_places_nearby.json | wc -l</code></pre>
<p>grep is particularly fast at searching plaintext files, such as the JSONs created by <em>gmaps-scraper</em>:</p>
<pre><code>$ file=output/json/2016-12-27_New_York_New_York_places_nearby.json 
$ du $file
359M    output/json/2016-12-27_New_York_New_York_places_nearby.json
$ time grep atm $file | wc -l
10082

real    0m5.934s
user    0m0.415s
sys 0m0.357s</code></pre>
